import numpy as np
import scipy.linalg as linalg
from scipy.linalg.lapack import dtrtri

# Fix the random seed
np.random.seed(1000)

# -----------------------------------------------------------------------
# Define a helper function L, which computes the performance of the gain vector g 
# given the parameters of the environment. 

def L(g,w,mu,P,W,alpha):
    # L takes in:
    # g: a vector of gains
    # w: a vector of pre-modulated spike counts
    # mu: the information-energy trade-off parameter
    # P: the covariance matrix of the population response
    # W: the noise correlation matrix of the population response
    # alpha: the power law exponent of the firing rate distribution

    # L returns the performance of the gain vector g on the environment defined by w, mu, P, W, and alpha.
    
    # Compute the diagonal matrix ra = diag( (g*w)^{alpha - 1} )
    ra = np.diag( np.power(w*g, alpha - 1 ) )
    # Compute the Cholesky decomposition of P + ra W ra
    B = np.linalg.cholesky( P + ra@W@ra )
    # Compute the value of the objective function and return it 
    return 2*(1 - alpha)*mu*np.sum(np.log( g*w )) +  2*mu*np.sum(np.log(np.diag(B))) - g.T@w

# ----------------------------------------------------------------------
# Define the parameters for the simulations

N = 10000 # This is the number of neuron clusters in the population
alpha = 1/2 # This is the power law exponent for the firing rate distribution
beta = 2*(1 - alpha)
sig2 = 1 # Define the base strength of noise.
# We divide by beta to keep the cluster firing rate constant at 10 per coding interval.
mu = 10/beta # This is the information-energy trade-off parameter.
CV = np.sqrt(10) # This is the coefficient of variation of each cluster, held constant.
CV2 = CV*CV # This is the square of the coefficient of variation.
p = 0.3 # This is the noise correlation between neurons in the population. 

# D and Dinv are n and 1/n spectrum matrices which we use in defining covariances later on. 
D = np.diag(1/np.array(range(1,N+1)))
Dinv = np.diag(np.array(range(1,N+1)))

# epsilon indexes the environments, taking values between 0 and 1.
# eps_res is the number of values for epsilon. epsilon is a linear sampling between 0 and 1 of epsilon values.  
eps_res = 21 #Previous value was 21
epsilon = np.linspace(0,1,eps_res)

# -----------------------------------------------------------------------
# Define containers for the simulation outputs

# These will be used to store the homeostatic and optimal solutions
g0 = np.zeros((N,eps_res)) 
g1 = np.zeros((N,eps_res))
g0hom = np.zeros((N,eps_res))
g_opt = np.zeros((N,eps_res))

# These are used to store the performance of L for g0, g1, and gopt respectively.
L0 = np.zeros((eps_res,1))
L0hom = np.zeros((eps_res,1))
L1 = np.zeros((eps_res,1))
Lopt = np.zeros((eps_res,1))

# This is the value of L for unadapted g0, g0hom, and g1 respectively. 
Lunadapted0 = np.zeros((eps_res,1))
Lunadapted0hom = np.zeros((eps_res,1))
Lunadapted1 = np.zeros((eps_res,1))

# C0 and C1 are the ``relative performance'' functions for g0, g0hom, and the first order approximation g1
C0 = np.zeros((eps_res-1,1))
C1 = np.zeros((eps_res-1,1))
C0hom = np.zeros((eps_res-1,1))

# These are used to track the relative error of the g0, g0hom, and g1 solutions compared to the optimal solution. 
g0err = np.zeros((eps_res,1))
g1err = np.zeros((eps_res,1))
g0homerr = np.zeros((eps_res,1))

# For each value of epsilon we define a collection of N by N matrices as follows:
S = np.zeros((N,N)) # S is a symmetric matrix
V = np.zeros((N,N)) # S can be diagonalised as V L V^T where V is orthogonal
Sigma = np.zeros((N,N))  # A covariance matrix Sigma is generated by V D V^T 
std = np.zeros((N,)) # std is the square root of the diagonal of Sigma
std_inv = np.zeros((N,)) # std_inv is the inverse of std
rho = np.zeros((N,N)) # A correlation rho is then given by std_inv Sigma std_inv
rho_inv = np.zeros((N,N)) # This is the inverse of rho 
P = np.zeros((N,N)) # P = CV^2(rho/sigma^2) 
W = (1 - p)*np.eye(N) + p*np.ones((N,N)) # W is the noise correlation matrix

# w are the pre-modulated spike count values. 
w = np.zeros((N,eps_res))

# -----------------------------------------------------------------------
# Sample the environments 

# We first sample the end points from two beta(6, 1) distributions
w[:,0] = np.reshape(np.random.beta(6,1,size = (N,1)),(N,))
w[:,eps_res-1] = np.reshape(np.random.beta(6,1,size = (N,1)),(N,))
# We then linearly interpolate to find w values for intermediate epsilon values. 
for k in range(1,eps_res-1):
    w[:,k] = epsilon[k]*w[:,eps_res-1] + (1 - epsilon[k])*w[:,0]

# Generate two random Gaussian matrices and add them to their transposes to create random symmetric matrices. 
R0 = np.random.randn(N,N)
S0 = R0.T + R0
R1 = np.random.randn(N,N)
S1 = R1.T + R1

# -----------------------------------------------------------------------
# Compute the homeostatic solutions scaling chi.

# We compute q
q = sig2*(1 - p)/(mu*CV2)
# Next we compte chi
chi = mu*q*np.log(N)/( (N**q) - 1 )

# -----------------------------------------------------------------------
# Now we compute the optimal solutions for each value of epsilon.
# We also find the performance of the unadapted solutions g0, g0hom, and g1 on each environment.


# Iterate through values of epsilon.
for k in range(eps_res):
    print('epsilon = ' + str(epsilon[k]))

    # -----------------------------------------------------------------------
    # Generate the environmental parameters for the k-th environment.
    
    # Create the symmetric matrix S = S(epsilon) by linearly interpolating S1 and S0
    S = epsilon[k]*S1 + (1 - epsilon[k])*S0
    # Find the orthogonal matrix V by taking the svd of S
    eigs, V = linalg.eigh(S)
    # Compute Sigma = V D V^T a +ve-definite covariance
    Sigma = V@D@V.T
    # Compute std and std_inv as normalising factors
    std =  np.sqrt(np.diag(Sigma)) 
    std_inv = 1/std
    # Compute rho = std^{-1} Sigma std^{-1}
    rho = np.diag(std_inv)@Sigma@np.diag(std_inv)
    # Compute rho_inv = std V Dinv V^T std
    rho_inv = np.diag(std)@V@Dinv@(V.T)@np.diag(std)
    # Finally, define P =  CV^2(rho/sigma^2)
    P = CV*CV*rho/sig2 

    # -----------------------------------------------------------------------
    # Compute analytic solution for the k-th environment.

    # g0 is the homeostatic approximate solution
    g0[:,k] = beta*mu/w[:,k]       
    # L0 is the performance of the homeostatic solution 
    L0[k] = L(g0[:,k],w[:,k],mu,P,W,alpha)
    # g0hom is the homeostatic solution with adjusted scaling mu*beta \mapsto chi
    g0hom[:,k] = chi/w[:,k]
    # L0hom is the performance of the homeostatic solution with adjusted scaling
    L0hom[k] = L(g0hom[:,k],w[:,k],mu,P,W,alpha)
    # g1 is new computed using our standard formula
    g1[:,k] = beta*mu*(1 - sig2*np.diag( rho_inv@W )/(CV*CV*pow(beta*mu, beta))  )/(w[:,k])
    # L1 is the performance of the gains g1
    L1[k] = L(g1[:,k],w[:,k],mu,P,W,alpha)
    print("L1 - L0: "+str(L1[k] - L0[k]))

    # -----------------------------------------------------------------------
    # Now we compute the optimal solution for the k-th environment using gradient ascent.
    
    # Compate the values of L0hom, L1, and L0. Initialise the gains to whichever is larger.
    # Also initialise Lold to the value of L corresponding to the initialisation.
    if L0hom[k] > L1[k]:
        if L0hom[k] > L0[k]:
            g = g0hom[:,k]
            Lold = L0hom[k]
        else:
            g = g0[:,k]
            Lold = L0[k]
    else:
        if L1[k] > L0[k]:
            g = g1[:,k]
            Lold = L1[k]
        else:
            g = g0[:,k]
            Lold = L0[k]
    
    ra = np.diag( np.power(w[:,k]*g, alpha - 1 ) )
    B = np.linalg.cholesky( P + ra@W@ra )
    Lold = beta*mu*np.sum(np.log( g*w[:,k] )) + 2*mu*np.sum(np.log(np.diag(B))) - g.T@w[:,k]
    T, low = dtrtri(B, lower = 1) 
    print("Initial value of L: "+ str(Lold))
    A = T.T@T
    gradL = (beta*mu)*np.diag(A@P)/g - w[:,k]
    negHess = (beta*mu)*np.diag(1/g)@(  alpha*(P@A)*np.eye(N) + (1 - alpha)*( (A@P)*(P@A) +(P@A@P)*A - P*A )   )@np.diag(1/g)
    step = linalg.cho_solve( linalg.cho_factor( negHess ), gradL )
    step_norm = np.linalg.norm(step)
    print("Intitial step norm: "+ str(step_norm))
    
    g1norm = np.linalg.norm(g1[:,k])
    lr = 1
    Ldiff = 1000
    print("L1 - L0: "+ str(L1[k] - L0[k]))

    # Iterate until (1) The step norm is sufficiently small, and (2) The change in L is sufficiently small.
    while step_norm > g1norm/10000 or Ldiff > abs(L1[k] - L0[k])/10000: 
        gnew = g + lr*step # Propose a new g value

        # Compute the new value of L on the basis of the proposed update to g. 
        ranew = np.diag( np.power(w[:,k]*gnew, alpha - 1 ) )
        Bnew = np.linalg.cholesky( P + ranew@W@ranew )
        Lnew = beta*mu*np.sum(np.log( gnew*w[:,k] )) + 2*mu*np.sum(np.log(np.diag(Bnew))) - gnew.T@w[:,k]
        
        # If the new L value is not smaller than the old L value, reduce the learning rate and try again. 
        if Lold - Lnew > 0:
            lr = lr/2
            print("Learning rate updated: "+str(lr))    
        # If the new L value is smaller than the old L value, accept the new g value and compute the next step.
        else:
            g = gnew # Accept the new g proposal
            Ldiff = Lnew - Lold # Compute the change in the objective as a result of the step. 
            print("Increase in L value: "+ str(Ldiff))
            Lold = Lnew # Update the objective function  
            print("Updated value of L: "+ str(Lnew))
            
            #Find the updated values of A, and via that the new step.
            T, low = dtrtri(Bnew, lower = 1)  
            A = T.T@T 
            # Compute the gradient of the objective function
            gradL = (beta*mu)*np.diag(A@P)/g - w[:,k] 
            # Compute the negative Hessian of the objective function
            negHess = (beta*mu)*np.diag(1/g)@(  alpha*(P@A)*np.eye(N) + (1 - alpha)*( (A@P)*(P@A) +(P@A@P)*A - P*A )   )@np.diag(1/g)
            # Compute the step 
            step = linalg.cho_solve( linalg.cho_factor( negHess ), gradL )
            # Compute the norm of the step
            step_norm = np.linalg.norm(step)
            print("step norm: "+ str(step_norm))
        
    #Once convergence is reached:
    g_opt[:,k] = gnew
    Lopt[k] = Lold
    
    # These compute the performance of the unadapted gains g0 and g1 on the k-th values of w. 
    Lunadapted0[k] = L(g0[:,0],w[:,k],mu,P,W,alpha)
    Lunadapted1[k] = L(g1[:,0],w[:,k],mu,P,W,alpha)
    Lunadapted0hom[k] = L(g0hom[:,0],w[:,k],mu,P,W,alpha)
    
    # Compute the relative error of the g0, g1, and g0hom solutions compared to the optimal solution.
    g0err[k] = np.mean(np.abs(g_opt[:,k] - g0[:,k])/g_opt[:,k])
    g1err[k] =  np.mean(np.abs(g_opt[:,k] - g1[:,k])/g_opt[:,k])
    g0homerr[k] = np.mean(np.abs(g_opt[:,k] - g0hom[:,k])/g_opt[:,k])
    
print("Mean relative error from g0: "+ str(np.mean(g0err)))
print("Mean relative error from g1: "+ str(np.mean(g1err)))
print("Mean relative error from g0hom: "+ str(np.mean(g0homerr)))

# Compute the performance metrics C0 and C1
C0 = (L0[1:] - Lunadapted0[1:])/(Lopt[1:] - Lunadapted0[1:])
C1 = (L1[1:] - Lunadapted1[1:])/(Lopt[1:] - Lunadapted1[1:])
C0hom = (L0hom[1:] - Lunadapted0hom[1:])/(Lopt[1:] - Lunadapted0hom[1:])

# Take all of the simulation output and save them using np.save, appending their names with the 
# number of the simulation and the value of alpha.

np.save("g0_p"+str(p)+".npy", g0)
np.save("g1_p"+str(p)+".npy", g1)
np.save("g0hom_p"+str(p)+".npy", g0hom)
np.save("g_opt_p"+str(p)+".npy", g_opt)
np.save("L0_p"+str(p)+".npy", L0)
np.save("L1_p"+str(p)+".npy", L1)
np.save("L0hom_p"+str(p)+".npy", L0hom)
np.save("Lopt_p"+str(p)+".npy", Lopt)
np.save("Lunadapted0_p"+str(p)+".npy", Lunadapted0)
np.save("Lunadapted1_p"+str(p)+".npy", Lunadapted1)
np.save("Lunadapted0hom_p"+str(p)+".npy", Lunadapted0hom)
np.save("C0_p"+str(p)+".npy", C0)
np.save("C1_p"+str(p)+".npy", C1)
np.save("C0hom_p"+str(p)+".npy", C0hom)
np.save("g0err_p"+str(p)+".npy", g0err)
np.save("g1err_p"+str(p)+".npy", g1err)
np.save("g0homerr_p"+str(p)+".npy", g0homerr)